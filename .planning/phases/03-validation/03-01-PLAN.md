---
phase: 03-validation
plan: 01
type: execute
---

<objective>
Validate MILP solver correctness by verifying checkpoint vessel costs against SOP expected values and confirming fleet-level constraint satisfaction.

Purpose: Ensure the MILP solver produces correct, trustworthy results before proceeding to sensitivity analyses and submission.
Output: Validation test suite (tests/test_validation.py) and all tests passing.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior summaries (direct dependencies):
@.planning/phases/01-clean-slate-data-foundation/01-02-SUMMARY.md
@.planning/phases/02-milp-optimizer/02-01-SUMMARY.md
@.planning/phases/02-milp-optimizer/02-02-SUMMARY.md

# Key source files:
@src/optimization.py
@src/constants.py
@src/data_adapter.py
@tests/fixtures/checkpoint_vessels.csv
@tests/test_milp.py
@run.py

**Tech stack available:** pulp, pandas, pytest
**Established patterns:** binary MILP fleet selection, linearized avg constraint, pytest.approx for numeric checks, fixture-based testing
**Constraining decisions:**
- Phase 02-01: Linearized safety constraint sum(safety_i - threshold) >= 0
- Phase 02-01: Return empty list on infeasible
- Phase 02-02: Column defaults final_cost, FC_total, CO2eq
- Phase 01-02: load_per_vessel() auto-falls back to test fixtures
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validation test suite for checkpoint vessel costs and MILP solution quality</name>
  <files>tests/test_validation.py</files>
  <action>
Create tests/test_validation.py with tests verifying MILP correctness on the 5 checkpoint vessels:

**Per-vessel cost checks (from checkpoint_vessels.csv):**
- 10102950: final_cost=880688, CO2eq=574.53, fuel_type=DISTILLATE FUEL, dwt=175108
- 10657280: final_cost=1260216, CO2eq=143.08, fuel_type=Ammonia, dwt=206331
- 10791900: final_cost=1043965, CO2eq=548.51, fuel_type=LNG, dwt=179700
- 10522650: final_cost=1156134, CO2eq=548.38, fuel_type=Methanol, dwt=115444
- 10673120: final_cost=1185540, CO2eq=103.67, fuel_type=Hydrogen, dwt=178838

**Test classes:**

1. `TestCheckpointVesselValues` — For each of the 5 vessels, verify final_cost and CO2eq match expected values exactly (integer comparison for cost, pytest.approx rel=1e-3 for CO2eq). Use a @pytest.fixture loading checkpoint_vessels.csv.

2. `TestMILPOnFixtures` — Run select_fleet_milp() on the 5 fixtures:
   - With demand=855421 (total DWT), safety>=1.0, fuel_diversity=False: all 5 selected, total cost=5526543
   - With demand=500000, safety>=3.0, fuel_diversity=False: 3 vessels selected, cost=3110193
   - After selection, call validate_fleet() and assert (ok=True, no errors)
   - After selection, call total_cost_and_metrics() and verify total_dwt >= demand, avg_safety >= threshold, fleet_size matches len(selected_ids)

3. `TestMetricsConsistency` — Run total_cost_and_metrics on all-5 selection and verify:
   - total_dwt == 855421 (sum of all DWT)
   - total_cost_usd == 5526543
   - fleet_size == 5
   - num_unique_main_engine_fuel_types == 5
   - avg_safety_score == 3.0 (mean of [1,3,5,3,3])
   - total_co2e_tonnes == pytest.approx(574.53+143.08+548.51+548.38+103.67, rel=1e-3)

Use the same patterns as test_milp.py: sys.path.insert, vessel fixture from CSV, class-based organization.
  </action>
  <verify>pytest tests/test_validation.py -v — all tests pass</verify>
  <done>All validation tests pass, confirming checkpoint vessel values match SOP and MILP produces correct fleet selections with valid constraints</done>
</task>

<task type="auto">
  <name>Task 2: Run full validation and verify all 18+ existing tests still pass</name>
  <files>run.py</files>
  <action>
Run the complete test suite to verify no regressions:

1. Run `pytest -v` — all existing tests (test_milp.py, test_optimization.py, test_cost_model.py, test_data_adapter.py) plus new test_validation.py must pass.
2. Run `python run.py` to verify end-to-end pipeline still works on fixtures (prints fleet selection results, no errors).
3. If any test fails, investigate and fix the failing test (not the source code — if source is wrong, that's a bigger issue to flag).

Do NOT modify any source files in src/ unless a genuine bug is discovered. This task is verification only.
  </action>
  <verify>pytest -v shows all tests passing (0 failures), python run.py exits 0 with fleet results printed</verify>
  <done>Full test suite passes (all existing + new validation tests), run.py produces correct output on fixtures, no regressions</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `pytest -v` passes all tests (existing + new validation)
- [ ] `python run.py` completes without error
- [ ] Checkpoint vessel costs verified against SOP values
- [ ] MILP constraint satisfaction confirmed via validate_fleet()
- [ ] Metrics consistency confirmed (DWT, cost, safety, fuel types)
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or regressions introduced
- Checkpoint vessel values confirmed correct
- MILP fleet selections validated against known expected outcomes
- Full test suite green
  </success_criteria>

<output>
After completion, create `.planning/phases/03-validation/03-01-SUMMARY.md`
</output>
